// API callback
random_posts({"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$blogger":"http://schemas.google.com/blogger/2008","xmlns$georss":"http://www.georss.org/georss","xmlns$gd":"http://schemas.google.com/g/2005","xmlns$thr":"http://purl.org/syndication/thread/1.0","id":{"$t":"tag:blogger.com,1999:blog-4433261812355873863"},"updated":{"$t":"2015-09-16T10:54:08.518-07:00"},"category":[{"term":"CUDA Programming Concept"},{"term":"CUDA Basics"},{"term":"CUDA Advance"},{"term":"Optimization in CUDA"},{"term":"CUDA programs Level 1.1"},{"term":"Books on CUDA"},{"term":"Images Processing"},{"term":"CUDA programs Level 1.2"},{"term":"CUDA programs Level 2.1"},{"term":"Compilation"},{"term":"Matlab Coding"},{"term":"C program"},{"term":"Debugging"},{"term":"Installation"},{"term":"CUDA Function"},{"term":"Kepler Features"}],"title":{"type":"text","$t":"CUDA Programming"},"subtitle":{"type":"html","$t":"The Complexity of the Problem is the Simplicity of the Solution "},"link":[{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"http:\/\/cuda-programming.blogspot.com\/feeds\/posts\/default"},{"rel":"self","type":"application/atom+xml","href":"http:\/\/www.blogger.com\/feeds\/4433261812355873863\/posts\/default?alt=json-in-script\u0026start-index=60\u0026max-results=1"},{"rel":"alternate","type":"text/html","href":"http:\/\/cuda-programming.blogspot.com\/"},{"rel":"hub","href":"http://pubsubhubbub.appspot.com/"},{"rel":"previous","type":"application/atom+xml","href":"http:\/\/www.blogger.com\/feeds\/4433261812355873863\/posts\/default?alt=json-in-script\u0026start-index=59\u0026max-results=1"},{"rel":"next","type":"application/atom+xml","href":"http:\/\/www.blogger.com\/feeds\/4433261812355873863\/posts\/default?alt=json-in-script\u0026start-index=61\u0026max-results=1"}],"author":[{"name":{"$t":"Nitin Gupta"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"32","height":"32","src":"\/\/lh5.googleusercontent.com\/-y8J34_QMdZA\/AAAAAAAAAAI\/AAAAAAAAACk\/yRljcINNhbw\/s512-c\/photo.jpg"}}],"generator":{"version":"7.00","uri":"http://www.blogger.com","$t":"Blogger"},"openSearch$totalResults":{"$t":"70"},"openSearch$startIndex":{"$t":"60"},"openSearch$itemsPerPage":{"$t":"1"},"entry":[{"id":{"$t":"tag:blogger.com,1999:blog-4433261812355873863.post-5067477156512586778"},"published":{"$t":"2012-12-25T07:31:00.004-08:00"},"updated":{"$t":"2013-01-12T05:23:50.417-08:00"},"category":[{"scheme":"http://www.blogger.com/atom/ns#","term":"CUDA Basics"},{"scheme":"http://www.blogger.com/atom/ns#","term":"CUDA Programming Concept"}],"title":{"type":"text","$t":"Thread Hierarchy in CUDA Programming"},"content":{"type":"html","$t":"\u003Cdiv dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\"\u003E\u003Cbr \/\u003E\u003Cdiv style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"\u003E\u003C\/div\u003E\u003Cbr \/\u003E\u003Cb style=\"background-color: white; color: #666666; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cu\u003E\u003Cspan style=\"font-size: large;\"\u003EBasic of CUDA Programming: Part 6\u003C\/span\u003E\u003C\/u\u003E\u003C\/b\u003E\u003Cbr \/\u003E\u003Cspan style=\"background-color: white; color: #666666; margin: 0px; padding: 0px;\"\u003E\u003Cspan style=\"margin: 0px; padding: 0px;\"\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cdiv style=\"text-align: center;\"\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif; font-size: large;\"\u003E\u003Cb\u003E\u003Cu\u003EThread Hierarchy in CUDA Programming\u003C\/u\u003E\u003C\/b\u003E\u003C\/span\u003E\u003C\/div\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003E\u003Ca href=\"http:\/\/www.caam.rice.edu\/~timwar\/RMMC\/CUDA_files\/screenshot_316.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" src=\"http:\/\/www.caam.rice.edu\/~timwar\/RMMC\/CUDA_files\/screenshot_316.png\" \/\u003E\u003C\/a\u003E\u003C\/div\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EFor convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003E\u003Ca href=\"http:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0010465510000159-gr001.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003E\u003Cimg border=\"0\" src=\"http:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0010465510000159-gr001.jpg\" \/\u003E\u003C\/a\u003E\u003C\/div\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EThe index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cimg height=\"440\" src=\"http:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S1093326310000914-gr2.jpg\" width=\"640\" \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EAs an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C:\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\/\/ Kernel definition\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E{\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;int i = threadIdx.x;\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;int j = threadIdx.y;\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;C[i][j] = A[i][j] + B[i][j];\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E}\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003Eint main()\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E{\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp;... \/\/ Kernel invocation with one block of N * N * 1 threads\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;int numBlocks = 1;\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;dim3 threadsPerBlock(N, N);\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;MatAdd\u0026lt;\u0026lt;\u0026lt;numBlocks, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(A, B, C);\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E\u0026nbsp;...\u0026nbsp;\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cspan style=\"color: blue;\"\u003E}\u003C\/span\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EThere is a limit to the number of threads per block (Know about this limit, \u003Ca href=\"http:\/\/en.wikipedia.org\/wiki\/CUDA\"\u003Eclick here\u003C\/a\u003E), since all threads of a block are expected to reside on the same processor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EHowever, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EBlocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by \u003Cb\u003EFigures\u003C\/b\u003E\u0026nbsp;The number of thread blocks in a grid is usually dictated by the size of the data being processed or the number of processors in the system, which it can greatly exceed.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cimg height=\"640\" src=\"http:\/\/www.cs.utexas.edu\/~fussell\/courses\/cs384g\/projects\/final\/artifacts_f08\/mjeong\/cuda-raytracer_files\/gridofthreadblocks.JPG\" width=\"513\" \/\u003E\u003Cimg height=\"400\" src=\"http:\/\/www2.engr.arizona.edu\/~yangsong\/gpu2.png\" width=\"640\" \/\u003E\u003Cbr \/\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EThe number of threads per block and the number of blocks per grid specified in the \u0026lt;\u0026lt;\u0026lt;â€¦\u0026gt;\u0026gt;\u0026gt; syntax can be of type int or dim3. Two-dimensional blocks or grids can be specified as in the example above.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EEach block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EExtending the previous MatAdd() example to handle multiple blocks, the code becomes as follows.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\/\/ Kernel definition\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E{\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp; \u003Cb\u003E\u0026nbsp;int i = blockIdx.x * blockDim.x + threadIdx.x;\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cb\u003E\u0026nbsp; \u0026nbsp;\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cb\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp;int j = blockIdx.y * blockDim.y + threadIdx.y;\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp;\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp;if (i \u0026lt; N \u0026amp;\u0026amp; j \u0026lt; N)\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp;\u003C\/span\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;C[i][j] = A[i][j] + B[i][j];\u003C\/span\u003E\u003Cbr \/\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E}\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003Eint main()\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E{\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E...\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\/\/ Kernel invocation\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cb\u003E\u0026nbsp; \u0026nbsp; dim3 threadsPerBlock(16, 16);\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cb\u003E\u0026nbsp; \u0026nbsp;\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cb\u003E\u0026nbsp; \u0026nbsp;dim3 numBlocks(N \/ threadsPerBlock.x, N \/ threadsPerBlock.y);\u0026nbsp;\u003C\/b\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp; \u0026nbsp; MatAdd\u0026lt;\u0026lt;\u0026lt;numBlocks, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(A, B, C);\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp;...\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"color: blue; font-family: Arial, Helvetica, sans-serif;\"\u003E}\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EA thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case.\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003EFeel free to comment...\u0026nbsp;\u003C\/span\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u003Cbr \/\u003E\u003C\/span\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif;\"\u003E\u0026nbsp;References \u003C\/span\u003E\u003Cbr \/\u003E\u003Cbr \/\u003E\u003Ca href=\"http:\/\/cuda-programming.blogspot.in\/2012\/12\/cuda-c-programming-guide.html\" style=\"background-color: white; border: 0px; color: #f43a40; margin: 0px; outline: none; padding: 0px; text-align: justify; text-decoration: initial; vertical-align: baseline;\"\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif; font-size: x-small;\"\u003ECUDA C Programming Guide\u003C\/span\u003E\u003C\/a\u003E\u003Cbr \/\u003E\u003Cspan style=\"font-family: Arial, Helvetica, sans-serif; font-size: x-small;\"\u003E\u003Ca href=\"http:\/\/docs.nvidia.com\/cuda\/index.html\"\u003E\u003Cspan id=\"goog_76350698\"\u003E\u003C\/span\u003ECUDA; Nvidia\u003C\/a\u003E\u003C\/span\u003E\u003Cbr \/\u003E\u003Ca href=\"http:\/\/en.wikipedia.org\/wiki\/CUDA\"\u003EWikipedia\u003C\/a\u003E\u0026nbsp;\u003C\/div\u003E"},"link":[{"rel":"replies","type":"application/atom+xml","href":"http:\/\/cuda-programming.blogspot.com\/feeds\/5067477156512586778\/comments\/default","title":"Post Comments"},{"rel":"replies","type":"text/html","href":"http:\/\/cuda-programming.blogspot.com\/2012\/12\/thread-hierarchy-in-cuda-programming.html#comment-form","title":"0 Comments"},{"rel":"edit","type":"application/atom+xml","href":"http:\/\/www.blogger.com\/feeds\/4433261812355873863\/posts\/default\/5067477156512586778"},{"rel":"self","type":"application/atom+xml","href":"http:\/\/www.blogger.com\/feeds\/4433261812355873863\/posts\/default\/5067477156512586778"},{"rel":"alternate","type":"text/html","href":"http:\/\/cuda-programming.blogspot.com\/2012\/12\/thread-hierarchy-in-cuda-programming.html","title":"Thread Hierarchy in CUDA Programming"}],"author":[{"name":{"$t":"Nitin Gupta"},"uri":{"$t":"https:\/\/plus.google.com\/118135070601300565447"},"email":{"$t":"noreply@blogger.com"},"gd$image":{"rel":"http://schemas.google.com/g/2005#thumbnail","width":"16","height":"16","src":"http:\/\/img2.blogblog.com\/img\/b16-rounded.gif"}}],"thr$total":{"$t":"0"}}]}});